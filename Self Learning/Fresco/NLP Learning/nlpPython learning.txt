rA. Intro
1.
Welcome to the course on NLP Using Python. In this course, you will get to learn the following topics:
Tokenizing text using functions word_tokenize and sent_tokenize.
Computing Frequencies with FreqDist and ConditionalFreqDist.
Generating Bigrams and collocations with bigrams and collocations.
Stemming word affixes using PorterStemmer and LancasterStemmer.
Tagging words to their parts of speech using pos_tag.
--------------------------------------------------------------------------
2.
Nothing
--------------------------------------------------------------------------
3.
NLP techniques are capable of processing and extracting meaningful insights, from huge unstructured data available online.
It can automate translating text from one language to other.
These techniques can be used for performing sentiment analysis.
It helps in building applications that interact with humans as humans do.
Also, NLP can help in automating Text Classification, Spam Filtering, and more.
--------------------------------------------------------------------------
4.
nltk is a popular Python framework used for developing Python programs to work with human language data.
Key features of nltk:

It provides access to over 50 text corpora and other lexical resources.
It is a suite of text processing tools.
It is free to use and Open source.
It is available for Windows, Mac OS X, and Linux.
--------------------------------------------------------------------------
5.
Nothing spl
--------------------------------------------------------------------------
6.
Now let's understand by performing simple tasks in the next couple of slides.

Splitting a sample text into a list of sentences.
import nltk
text = "Python is an interpreted high-level programming language for general-purpose programming. Created by Guido van Rossum and first released in 1991."
sentences = nltk.sent_tokenize(text)
len(sentences) - 2
As seen above, sent_tokenize function generates sentences from the given text.
--------------------------------------------------------------------------
7.
Splitting a sample text into words using word_tokenizer function.
words = nltk.word_tokenize(text)
len(words) - 22
words[:5] -  ['Python', 'is', 'an', 'interpreted', 'high-level']
The expression words[:5] displays first five words of list words.
--------------------------------------------------------------------------
8.
Determining the frequency of words present in sample text using FreqDist function.
wordfreq = nltk.FreqDist(words)
wordfreq.most_common(2)  - [('programming', 2), ('.', 2)]
The expression wordfreq.most_common(2) displays two highly frequent words with their respective frequency count.
--------------------------------------------------------------------------
9.
In this course, you will be coordinating with several texts curated by NLTK authors.
These texts are available in collection book of nltk.
They can be downloaded by running the following command in Python interpreter, after importing nltk successfully.
import nltk
nltk.download('book')
--------------------------------------------------------------------------
10.
Nothing
--------------------------------------------------------------------------
11.
Searching Text:
There are multiple ways of searching for a pattern in a text.
The example shown below searches for words starting with tri, and ending with r.
text1.findall("<tri.*r>")
triangular; triangular; triangular; triangular
-------------------------------------------------------------------------------------------------------------------
B. 
1.
In this topic, you will understand how to perform the following activities, using text1 as input text.
Total Word Count
Unique Word Count
Transforming Words
Word Coverage
Filtering Words
Frequency Distribution
--------------------------------------------------------------------------
2.
Determining Total Word Count

The text1, imported from nltk.book is an object of nltk.text.Text class.
from nltk.book import *
type(text1)
<class 'nltk.text.Text'>
Total number of words in text1 is determined using len.
n_words = len(text1)
n_words - 260819
--------------------------------------------------------------------------
3.
Determining Unique Word Count

A unique number of words in text1 is determined using set and len methods.
n_unique_words = len(set(text1))
n_unique_words - 19317
set(text1) generates list of unique words from text1.
--------------------------------------------------------------------------
4.

Transforming Words

It is possible to apply a function to any number of words and transform them.
Now let's transform every word of text1 to lowercase and determine unique words once again.

text1_lcw = [ word.lower() for word in set(text1) ]
n_unique_words_lc = len(set(text1_lcw))
n_unique_words_lc - 17231

A difference of 2086 can be found from n_unique_words.
--------------------------------------------------------------------------
5.
Determining Word Coverage

Word Coverage: Word Coverage refers to an average number of times a word is occurring in the text.

The following examples determine Word Coverage of raw and transformed text1.
word_coverage1 = n_words / n_unique_words
word_coverage1 - 13.502044830977896
On average, a single word in text1 is repeated 13.5 times.
word_coverage2 = n_words / n_unique_words_lc
word_coverage2 - 15.136614241773549
--------------------------------------------------------------------------
6.
Filtering Words

Now let's see how to filter words based on specific criteria.
The following example filters words having characters more than 17.

big_words = [word for word in set(text1) if len(word) > 17 ]
big_words - ['uninterpenetratingly', 'characteristically']
A list of comprehension with a condition is used above.
--------------------------------------------------------------------------
7.
Filtering Words - cont...

Now let's see one more example which filters words having the prefix Sun.
sun_words = [word for word in set(text1) if word.startswith('Sun') ]
sun_words - ['Sunday', 'Sunset', 'Sunda']
The above example is case-sensitive. It doesn't filter the words starting with lowercase s and followed by un.
--------------------------------------------------------------------------
8.
Frequency Distribution

FreqDist functionality of nltk can be used to determine the frequency of all words, present in an input text.

The following example, determines frequency distribution of text1 and further displays the frequency of word Sunday.

text1_freq = nltk.FreqDist(text1)
text1_freq['Sunday']  - 7
--------------------------------------------------------------------------
9.
image
--------------------------------------------------------------------------
10.
Frequency Distribution

Now let's identify three frequent words from text1_freq distribution using most_common method.
top3_text1 = text1_freq.most_common(3)
top3_text1 - [(',', 18713), ('the', 13721), ('.', 6862)]
The output says the three most frequent words are , , the, and .
--------------------------------------------------------------------------
11.

-------------------------------------------------------------------------------------------------------------------------------------------
B.
1. Conditional Frequency:

In the previous topic, you have studied about Frequency Distributions.
FreqDist function computes the frequency of each item in a list.
While computing a frequency distribution, you observe occurrence count of an event.

items = ['apple', 'apple', 'kiwi', 'cabbage', 'cabbage', 'potato']
nltk.FreqDist(items)
FreqDist({'apple': 2, 'cabbage': 2, 'kiwi': 1, 'potato': 1})
----------------------------------------------------------------------------
2.
A Conditional Frequency is a collection of frequency distributions, computed based on a condition.
For computing a conditional frequency, you have to attach a condition to every occurrence of an event.
Let's consider the following list for computing Conditional Frequency.

c_items = [('F','apple'), ('F','apple'), ('F','kiwi'), ('V','cabbage'), ('V','cabbage'), ('V','potato') ]

Each item is grouped either as a fruit F or a vegetable V
----------------------------------------------------------------------------
3.
Computing Conditional Frequency
ConditionalFreqDist function of nltk is used to compute Conditional Frequency Distribution (CDF).

The same can be viewed in the following example.

cfd = nltk.ConditionalFreqDist(c_items)
cfd.conditions()
['V', 'F']
cfd['V']
FreqDist({'cabbage': 2, 'potato': 1})
cfd['F']
FreqDist({'apple': 2, 'kiwi': 1})
-----------------------------------------------------------------------------
4.
Counting Words by Genre
Now let's determine the frequency of words, of a particular genre, in brown corpus.
cfd = nltk.ConditionalFreqDist([ 
(genre, word) 

for genre in brown.categories() 
for word in brown.words(categories=genre) ])

The conditions applied can be viewed as shown below.
cfd.conditions()
['adventure', 'hobbies', ...]
-------------------------------------------------------------------------------
5.
Viewing Word Count

Once after computing conditional frequency distribution, tabulate method is used for viewing the count along with arguments conditions and samples.
cfd.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'])

           leadership  worship  hardship 
government         12        3         2 
     humor          1        0         0 
   reviews         14        1         2 
-------------------------------------------------------------------------------
Viewing Cumulative Word Count
The cumulative count for different conditions is found by setting cumulative argument value to True.
cfd.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'], cumulative = True)

           leadership   worship  hardship 
government         12       15        17 
     humor          1        1         1 
   reviews         14       15        17 
---------------------------------------------------------------------------------
Accessing Individual Frequency Distributions
From the obtained conditional frequency distribution, you can access individual frequency distributions.

The below example extracts frequency distribution of words present in news genre of brown corpus.

news_fd = cfd['news']
news_fd.most_common(3)
[('the', 5580), (',', 5188), ('.', 4030)]
You can further access count of any sample as shown below.
news_fd['the']
5580
---------------------------------------------------------------------------------
Comparing Frequency Distributions
Now let's see another example, which computes the frequency of last character appearing in all names associated with males and females respectively and compares them.

The text corpus names contain two files male.txt and female.txt
--------------------------------------------------------------------------------
Comparing Frequency Distributions
from nltk.corpus import names
nt = [(fid.split('.')[0], name[-1])    for fid in names.fileids() 
  for name in names.words(fid) ]
 cfd2 = nltk.ConditionalFreqDist(nt)
cfd2['female'] > cfd2['male']
True
The expression cfd2['female'] > cfd2['male'] checks if the last characters in females occur more frequently than the last characters in males.
--------------------------------------------------------------------------------
Comparing Frequency Distributions
The following code snippet displays frequency count of characters a and e in females and males, respectively.
>>> cfd2.tabulate(samples=['a', 'e'])
          a    e 
female 1773 1432 
  male   29  468 
You can observe a significant difference in frequencies of a and e.
---------------------------------------------------------------------------------
STEMMING
Stemming is a process of stripping affixes from words.

Stemming
More often, you normalize text by converting all the words into lowercase. This will treat both words The and the as same.
With stemming, the words playing, played and play will be treated as single word, i.e. play.


nltk comes with few stemmers.
The two widely used stemmers are Porter and Lancaster stemmers.
These stemmers have their own rules for string affixes.
The following example demonstrates stemming of word builders using PorterStemmer.

from nltk import PorterStemmer
porter = nltk.PorterStemmer()
porter.stem('builders')
builder
-------------------------------------------------------------------------------------------------------
Stemmers in nltk
Now let's see how to use LancasterStemmer and stem the word builders.
>>> from nltk import LancasterStemmer
>>> lancaster = LancasterStemmer()
>>> lancaster.stem('builders')
build
Lancaster Stemmer returns build whereas Porter Stemmer returns builder.
-------------------------------------------------------------------------------------------------------
Normalizing with Stemming
Let's consider the text collection, text1.

Let's first determine the number of unique words present in original text1.

Then normalize the text by converting all the words into lower case and again determine the number of unique words.

>>> from nltk.book import *
>>> len(set(text1))
19317
>>> lc_words = [ word.lower() for word in text1] 
>>> len(set(lc_words))
17231

-------------------------------------------------------------------------------------------------------------------------
Normalizing with Stemming
Now let's further normalize text1 with Porter Stemmer.
>>> from nltk import PorterStemmer
>>> porter = PorterStemmer()
>>> p_stem_words = [porter.stem(word) for word in set(lc_words) ]
>>> len(set(p_stem_words))
10927
The above output shows that, after normalising with Porter Stemmer, the text1 collection has 10927 unique words.
----------------------------------------------------------------------------------------------------------------------------
Normalising with Stemming
Now let's normalise with Lancaster stemmer and determine the unique words of text1.
>>> from nltk import LancasterStemmer
>>> lancaster = LancasterStemmer()
>>> l_stem_words = [lancaster.stem(word) for word in set(lc_words) ]
>>> len(set(l_stem_words))
9036
Applying Lancaster Stemmer to text1 collection resulted in 9036 words.
-------------------------------------------------------------------------------------------------------------------------
Understanding Lemma
Lemma is a lexical entry in a lexical resource such as word dictionary.

You can find multiple Lemma's with the same spelling. These are known as homonyms.

For example, consider the two Lemma's listed below, which are homonyms.

1. saw [verb] - Past tense of see
2. saw [noun] - Cutting instrument
-----------------------------------------------------------------------------------------------------------------------------
Lemmatization
nltk comes with WordNetLemmatizer. This lemmatizer removes affixes only if the resulting word is found in lexical resource, Wordnet.
>>> wnl = nltk.WordNetLemmatizer()
>>> wnl_stem_words = [wnl.lemmatize(word) for word in set(lc_words) ]
>>> len(set(wnl_stem_words))
15168 
WordNetLemmatizer is majorly used to build a vocabulary of words, which are valid Lemmas.
-----------------------------------------------------------------------------------------------------------------------------------
Lemmatization
nltk comes with WordNetLemmatizer. This lemmatizer removes affixes only if the resulting word is found in lexical resource, Wordnet.
>>> wnl = nltk.WordNetLemmatizer()
>>> wnl_stem_words = [wnl.lemmatize(word) for word in set(lc_words) ]
>>> len(set(wnl_stem_words))
15168 
WordNetLemmatizer is majorly used to build a vocabulary of words, which are valid Lemmas.
------------------------------------------------------------------------------------------------------------------------------------
POS Tagging
The method of categorizing words into their parts of speech and then labeling them respectively is called POS Tagging.

POS Tagger
A POS Tagger processes a sequence of words and tags a part of speech to each word.

pos_tag is the simplest tagger available in nltk.

The below example shows usage of pos_tag.

>>> import nltk
>>> text = 'Python is awesome.'
>>> words = nltk.word_tokenize(text)
>>> nltk.pos_tag(words)
[('Python', 'NNP'), 
 ('is', 'VBZ'),
 ('awesome', 'JJ'),
 ('.', '.')]
------------------------------------------------------------------------------------------------------------------------------------
POS_Tagger
The words Python, is and awesome are tagged to Proper Noun (NNP), Present Tense Verb (VB), and adjective (JJ) respectively.

You can read more about the pos tags with the below help command

>>> nltk.help.upenn_tagset()
To know about a specific tag like JJ, use the below-shown expression
>>> nltk.help.upenn_tagset('JJ')
JJ: adjective or numeral, ordinal
------------------------------------------------------------------------------------------------------------------------------------------
Tagging Text
Constructing a list of tagged words from a string is possible.

A tagged word or token is represented in a tuple, having the word and the tag.

In the input text, each word and tag are separated by /.

>>> text = 'Python/NN is/VB awesome/JJ ./.'
>>> [ nltk.tag.str2tuple(word) for word in text.split() ]
[('Python', 'NN'),
 ('is', 'VB'),
 ('awesome', 'JJ'),
 ('.', '.')]
--------------------------------------------------------------------------------------------------------------------------------------

Tagged Corpora
Many of the text corpus available in nltk, are already tagged to their respective parts of speech.

tagged_words method can be used to obtain tagged words of a corpus.

The following example fetches tagged words of brown corpus and displays few.

>>> from nltk.corpus import brown
>>> brown_tagged = brown.tagged_words(
>>> brown_tagged[:3]
[('The', 'AT'),
 ('Fulton', 'NP-TL'),
 ('County', 'NN-TL')]
------------------------------------------------------------------------------------------------------------------------
DefaultTagger
DefaultTagger assigns a specified tag to every word or token of given text.

An example of tagging NN tag to all words of a sentence, is shown below.

>>> import nltk
>>> text = 'Python is awesome.'
>>> words = nltk.word_tokenize(text)
>>> default_tagger = nltk.DefaultTagger('NN')
>>> default_tagger.tag(words)
[('Python', 'NN'),
 ('is', 'NN'),
 ('awesome', 'NN'),
 ('.', 'NN')]
--------------------------------------------------------------------------------------------------------------------------
Lookup Tagger
You can define a custom tagger and use it to tag words present in any text.

The below-shown example defines a dictionary defined_tags, with three words and their respective tags.

>>> import nltk
>>> text = 'Python is awesome.'
>>> words = nltk.word_tokenize(text)
>>> defined_tags = {'is':'BEZ', 'over':'IN', 'who': 'WPS'}
----------------------------------------------------------------------------------------------------------------------------
Lookup Tagger
The example further defines a UnigramTagger with the defined dictionary and uses it to predict tags of words in text.
>>> baseline_tagger = nltk.UnigramTagger(model=defined_tags)
>>> baseline_tagger.tag(words)
[('Python', None),
 ('is', 'BEZ'),
 ('awesome', None),
 ('.', None)]
Since the words Python and awesome are not found in defined_tags dictionary, they are tagged to None.
--------------------------------------------------------------------------------------------------------------------------
Unigram Tagger
UnigramTagger provides you the flexibility to create your taggers.

Unigram taggers are built based on statistical information. i.e., they tag each word or token to most likely tag for that particular word.

You can build a unigram tagger through a process known as training.

Then use the tagger to tag words in a test set and evaluate the performance.
----------------------------------------------------------------------------------------------------
Unigram Tagger
Let's consider the tagged sentences of brown corpus collections, associated with government genre.

Let's also compute the training set size, i.e., 80%.

>>> from nltk.corpus import brown
>>> brown_tagged_sents = brown.tagged_sents(categories='government')
>>> brown_sents = brown.sents(categories='government')
>>> len(brown_sents)
3032
>>> train_size = int(len(brown_sents)*0.8)
>>> train_size
2425
---------------------------------------------------------------------------------------------------------

Unigram Tagger
>>> train_sents = brown_tagged_sents[:train_size]
>>> test_sents = brown_tagged_sents[train_size:]
>>> unigram_tagger = nltk.UnigramTagger(train_sents)
>>> unigram_tagger.evaluate(test_sents)
0.7804399607678296
unigram_tagger is built by passing trained tagged sentences as argument to UnigramTagger.

The built unigram_tagger is further evaluated with test sentences.
-----------------------------------------------------------------------------------------------------------
Unigram Tagger
The following code snippet shows tagging words of a sentence, taken from the test set.
>>> unigram_tagger.tag(brown_sents[3000])
[('The', 'AT'),
 ('first', 'OD'),
 ('step', 'NN'),
 ('is', 'BEZ'),
 ('a', 'AT'),
 ('comprehensive', 'JJ'),
 ('self', None),
 ('study', 'NN'),
 ....
 ('.', '.')]
---------------------------------------------------------------------------------------------------------------
