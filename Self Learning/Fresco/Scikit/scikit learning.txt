A. Preporcessing

scikit-learn library has many utilities that can be used to perform the following tasks involved in Machine Learning.

Preprocessing
Model Selection
Classification
Regression
Clustering
Dimensionality Reduction


Mostly, one would perform the following steps while working on a Machine learning problem with scikit-learn:

Cleaning raw data set.
Further transforming with many scikit-learn pre-processing utilities.
Splitting data into train and test sets with train_test_split utility.
Creating a suitable model with default parameters.
Training the Model using fit function.
Evaluating the Model and fine-tuning it.

cancer_target = cancer_target.replace(['M', 'B'], [0, 1])

scikit-learn by default comes with few popular datasets.They can be loaded into your working environment and used.



scikit-learn provides many preprocessing utilities such as,

Standardization mean removal
1.Standardization or Mean Removal is the process of transforming each feature vector into a normal distribution with mean 0 and variance 1.
2.This can be achieved using StandardScaler.

	standardizer = preprocessing.StandardScaler()
	standardizer = standardizer.fit(breast_cancer.data)
	breast_cancer_standardized = standardizer.transform(breast_cancer.data)

2.Scaling
	Scaling transforms existing data values to lie between a minimum and maximum value.
	MinMaxScaler transforms data to range 0 and 1.
	MaxAbsScaler transforms data to range -1 and 1.

	min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 10)).fit(breast_cancer.data)
	breast_cancer_minmaxscaled10 = min_max_scaler.transform(breast_cancer.data)

	min_max_scaler = preprocessing.MinMaxScaler().fit(breast_cancer.data)
	breast_cancer_minmaxscaled = min_max_scaler.transform(breast_cancer.data)

3.Normalization
	Normalization scales each sample to have a unit norm.
	Normalization can be achieved with 'l1', 'l2', and 'max' norms.
	'l1' norm makes the sum of absolute values of each row as 1, and 'l2' norm makes the sum of squares of each 	row as 1.
	'l1' norm is insensitive to outliers.
	By default l2 norm is considered. Hence, removing outliers is recommended before applying l2 norm.
	normalizer = preprocessing.Normalizer(norm='l1').fit(breast_cancer.data)
	breast_cancer_normalized = normalizer.transform(breast_cancer.data)

4.Binarization
	Binarization is the process of transforming data points to 0 or 1 based on a given threshold.
	Any value above the threshold is transformed to 1, and any value below the threshold is transformed to 0.
	By default, a threshold of 0 is used.
	binarizer = preprocessing.Binarizer(threshold=3.0).fit(breast_cancer.data)
	breast_cancer_binarized = binarizer.transform(breast_cancer.data)
	print(breast_cancer_binarized[:5,:5])

5.One Hot Encoding
	OneHotEncoder converts categorical integer values into one-hot vectors. In an on-hot vector, every category 	is transformed into a binary attribute having only 0 and 1 values.
	onehotencoder = preprocessing.OneHotEncoder()
	onehotencoder = onehotencoder.fit([[1], [1], [1], [2], [2], [1]])
	# Transforming category values 1 and 2 to one-hot vectors
	print(onehotencoder.transform([[1]]).toarray())
	print(onehotencoder.transform([[2]]).toarray())

6.Label Encoding
	
Label Encoding is a step in which, in which categorical features are represented as categorical integers. An example of transforming categorical values ["benign","malignant"]into[0, 1]` is shown below.	

7.Imputation
	Imputation replaces missing values with either median, mean, or the most common value of the column or row 		in which the missing values exist.
	imputer = preprocessing.Imputer(missing_values='NaN', strategy='mean')
	imputer = imputer.fit(breast_cancer.data)
	breast_cancer_imputed = imputer.transform(breast_cancer.data)


-------------------------------------------------------------------------------------------------------------
B. Nearest Neighbor Technique

	Nearest neighbors method is used to determine a predefined number of data points that are closer to a 	sample point and predict its label.

sklearn.neighbors provides utilities for unsupervised and supervised neighbors-based learning methods.
scikit-learn implements two different nearest neighbors classifiers:
	KNeighborsClassifier
	RadiusNeighborsClassifier

KNeighborsClassifier classifies based on k nearest neighbors of every query point, where k is an integer value specified by the user.
KNeighborsRegressor predicts based on the k nearest neighbors of each query point.

Eg: 
	import sklearn.datasets as datasets
	from sklearn.model_selection import train_test_split
	from sklearn.neighbors import KNeighborsClassifier
	cancer = datasets.load_breast_cancer()  # Loading the data set
	X_train, X_test, Y_train, Y_test = train_test_split(cancer.data, cancer.target,
           stratify=cancer.target,random_state=42)
	knn_classifier = KNeighborsClassifier()   
	knn_classifier = knn_classifier.fit(X_train, Y_train) 
	print('Accuracy of Train Data :', knn_classifier.score(X_train,Y_train))
	print('Accuracy of Test Data :', knn_classifier.score(X_test,Y_test))
RadiusNeighborsClassifier classifies based on the number of neighbors present in a fixed radius r of every training point.
RadiusNeighborsRegressor predicts based on the neighbors present in a fixed radius r of the query point.

----------------------------------------------------------------------------------------------------------------
C. Decison Tree : 

Decision Trees is another Supervised Learning method used for Classification and Regression.
Decision Trees learn simple decision rules from training data and build a Model.
DecisionTreeClassifier and DecisionTreeRegressor are the two utilities from sklearn.tree, which can be used for classification and regression respectively.

Decision Trees are easy to understand.
They often do not require any preprocessing.
Decision Trees can learn from both numerical and categorical data.

Decision trees sometimes become complex, which do not generalize well and leads to overfitting. Overfitting can be addressed by placing the least number of samples needed at a leaf node or placing the highest depth of the tree.

A small variation in data can result in a completely different tree. This problem can be addressed by using decision trees within an ensemble.

from sklearn.tree import DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier()   
dt_classifier = dt_classifier.fit(X_train, Y_train)
print('Accuracy of Train Data :', dt_classifier.score(X_train,Y_train))
print('Accuracy of Test Data :', dt_classifier.score(X_test,Y_test)
o/p:
Accuracy of Train Data : 1.0
Accuracy of Test Data : 0.895104895105
Model is overfitted.

Max depth : improves the model
dt_classifier = DecisionTreeClassifier(max_depth=2)   
dt_classifier = dt_classifier.fit(X_train, Y_train) 
print('Accuracy of Train Data :', dt_classifier.score(X_train,Y_train))
print('Accuracy of Test Data :', dt_classifier.score(X_test,Y_test))

Accuracy of Train Data : 0.946009389671
Accuracy of Test Data : 0.923076923077

-------------------------------------------------------------------------------------------------------------
D. Ensemble Model


Ensemble methods combine predictions of other learning algorithms, to improve the generalization.

Ensemble methods are two types:

Averaging Methods: They build several base estimators independently and finally average their predictions.
E.g.: Bagging Methods, Forests of randomised trees
Boosting Methods: They build base estimators sequentially and trie to reduce the bias of the combined estimator.
E.g.: Adaboost, Gradient Tree Boosting

Bagging: 
Bagging Methods draw random subsets of the original dataset, build an estimator and aggregate individual results to form a final one.
BaggingClassifier and BaggingRegressor are the utilities from sklearn.ensemble to deal with Bagging.

sklearn.ensemble offers two types of algorithms based on randomized trees: Random Forest and Extra randomness algorithms.
RandomForestClassifier and RandomForestRegressor classes are used to deal with random forests.
In random forests, each estimator is built from a sample drawn with replacement from the training set.

ExtraTreesClassifier and ExtraTreesRegressor classes are used to deal with extremely randomized forests.
In extremely randomized forests, more randomness is introduced, which further reduces the variance of the model.

Boosting : 
Boosting Methods combine several weak models to create a improvised ensemble.
sklearn.ensemble also provides the following boosting algorithms:AdaBoostClassifier and GradientBoostingClassifier

Random forest Classifier : 
	from sklearn.ensemble import RandomForestClassifier
	rf_classifier = RandomForestClassifier()
	rf_classifier = rf_classifier.fit(X_train, Y_train) 
	print('Accuracy of Train Data :', rf_classifier.score(X_train,Y_train))
	print('Accuracy of Test Data :', rf_classifier.score(X_test,Y_test))


---------------------------------------------------------------------------------------------------
E. Understanding SVM:
	Support Vector Machines (SVMs) separates data points based on decision planes, which separates objects belonging to different classes in a higher dimensional space.
SVM algorithm uses the best suitable kernel, which is capable of separating data points into two or more classes.
Commonly used kernels are:
	linear
	polynomial
	rbf
	sigmoid

scikit-learn provides the following three utilities for performing Support Vector Classification.
SVC,
NuSVC: Same as SVC but uses a parameter to control the number of support vectors.
LinearSVC: Similar to SVC with parameter kernel taking linear value.

Adv:
SVM can distinguish the classes in a higher dimensional space.
SVM algorithms are memory efficient.
SVMs are versatile, and a different kernel can be used by a decision function.
DisAdv:
SVMs do not perform well on high dimensional data with many samples.
SVMs work better only with Preprocessed data.
They are harder to visualize.

Demo : 
from sklearn.svm import SVC
svm_classifier = SVC()
svm_classifier = svm_classifier.fit(X_train, Y_train) 
print('Accuracy of Train Data :', svm_classifier.score(X_train,Y_train))
print('Accuracy of Test Data :', svm_classifier.score(X_test,Y_test))
Accuracy of Train Data : 1.0
Accuracy of Test Data : 0.629370629371

import sklearn.preprocessing as preprocessing
standardizer = preprocessing.StandardScaler()
standardizer = standardizer.fit(cancer.data)
cancer_standardized = standardizer.transform(cancer.data)
svm_classifier = SVC()
svm_classifier = svm_classifier.fit(X_train, Y_train)
print('Accuracy of Train Data :', svm_classifier.score(X_train,Y_train))
print('Accuracy of Test Data :', svm_classifier.score(X_test,Y_test))
Accuracy of Train Data : 0.992957746479
Accuracy of Test Data : 0.979020979021

from sklearn import metrics
Y_pred = svm_classifier.predict(X_test)
print('Classification report : \n',metrics.classification_report(Y_test, Y_pred))

Classification report : 
      precision recall f1-score  support
0      0.96      0.98      0.97        53
1      0.99      0.98      0.98        90
avg    0.98      0.98      0.98       143

-------------------------------------------------------------------------------------------------------------------
Clustering: 

Clustering is one of the unsupervised learning technique.
The technique is typically used to group data points into clusters based on a specific algorithm.
Major clustering algorithms that can be implemented using scikit-learn are:

	K-means Clustering
	Agglomerative clustering
	DBSCAN clustering
	Mean-shift clustering
	Affinity propagation
	Spectral clustering

K-Means Clustering
In K-means Clustering entire data set is grouped into k clusters.
Steps involved are:
k centroids are chosen randomly.
The distance of each data point from k centroids is calculated. A data point is assigned to the nearest cluster.
Centroids of k clusters are recomputed.
The above steps are iterated till the number of data points a cluster reach convergence.
KMeans from sklearn.cluster can be used for K-means clustering.

Agglomerative Hierarchical Clustering
Agglomerative Hierarchical Clustering is a bottom-up approach.

Steps involved are:
Each data point is treated as a single cluster at the beginning.
The distance between each cluster is computed, and the two nearest clusters are merged together.
The above step is iterated till a single cluster is formed.
AgglomerativeClustering from sklearn.cluster can be used for achieving this.
Merging of two clusters can be any of the following linkage type: ward, complete or average.

Mean Shift Clustering aims at discovering dense areas.
Steps Involved:
Identify blob areas with randomly guessed centroids.
Calculate the centroid of each blob area and shift to a new one, if there is a difference.
Repeat the above step till the centroids converge.
make_blobs from sklearn.cluster can be used to initialize the blob areas. MeanShift from sklearn.cluster can be used to perform Mean Shift clustering.

Affinity Propagation generates clusters by passing messages between pairs of data points, until convergence.
AffinityPropagation class from sklearn.cluster can be used.
The above class can be controlled with two major parameters:
Preference: It controls the number of exemplars to be chosen by the algorithm.
damping: It controls numerical oscillations while updating messages.

Spectral Clustering is ideal to cluster data that is connected, and may not be in a compact space.
In general, the following steps are followed:
Build an affinity matrix of data points.
Embed data points in a lower dimensional space.
Use a clustering method like k-means to partition the points on lower dimensional space.
spectral_clustering from sklearn.cluster can be used for achieving this.

KMeans Demo : 

from sklearn.cluster import KMeans
kmeans_cluster = KMeans(n_clusters=2)
kmeans_cluster = kmeans_cluster.fit(X_train) 
kmeans_cluster.predict(X_test)

Evaluation: 
A clustering algorithm is majorly evaluated using the following scores:
Homogeneity: Evaluates if each cluster contains only members of a single class.
Completeness: All members of a given class are assigned to the same cluster.
V-measure: Harmonic mean of Homogeneity and Completeness.
Adjusted Rand index: Measures similarity of two assignments.


from sklearn import metrics
print(metrics.homogeneity_score(kmeans_cluster.predict(X_test), Y_test))
print(metrics.completeness_score(kmeans_cluster.predict(X_test), Y_test))
print(metrics.v_measure_score(kmeans_cluster.predict(X_test), Y_test))
print(metrics.adjusted_rand_score(kmeans_cluster.predict(X_test), Y_test))

0.573236466834
0.483862796607
0.524771531969
0.54983994112